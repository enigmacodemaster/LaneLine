{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h2b3rnQs1okx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qbORR9jNBoie"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OpwELwgKBvGw"
   },
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lWktzkQWBz9X"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chans, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = block\n",
    "        self.in_channels = 64\n",
    "        '''\n",
    "        修改conv1，使输出的feature_map与原图大小完全相同，注意此处要求将kernel_size修改为3，改进一下，可以放到Unet++中去\n",
    "        '''\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#             )\n",
    "        # 和原图一样的大小，采用half padding = kernel_size // 2\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ) \n",
    "        # 输出原图大小的feature map\n",
    "        # kernel_size改成3的话，参数量减少了，小卷积核研究现状更优于大卷积核\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.conv1(x) # 原图大小\n",
    "        f2 = self.conv2_x(self.pool(f1)) # 原图1/2\n",
    "        f3 = self.conv3_x(f2)\n",
    "        f4 = self.conv4_x(f3)\n",
    "        f5 = self.conv5_x(f4)\n",
    "        output = self.avg_pool(f5)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        '''\n",
    "        提取网络的3个中间层feature_map，要求第一个与原图大小相同，第二个为原图的1/2，第三个为原图的1/4\n",
    "        '''\n",
    "#         return output\n",
    "        return f1, f2, f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AOb6hrRJB2tK"
   },
   "outputs": [],
   "source": [
    "def resnet18(in_chans):\n",
    "    return ResNet(in_chans, BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34(in_chans):\n",
    "    return ResNet(in_chans, BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50(in_chans):\n",
    "    return ResNet(in_chans, BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101(in_chans):\n",
    "    return ResNet(in_chans, BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152(in_chans):\n",
    "    return ResNet(in_chans, BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-ER1eKcMIC-"
   },
   "source": [
    "### ResNet_UNetpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tNFnJElR528Y"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, in_chans, out_chans, stride):\n",
    "    super(ConvBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=3, stride=stride, padding=1)\n",
    "    self.bn1 = nn.BatchNorm2d(out_chans)\n",
    "    self.relu1 = nn.ReLU(inplace=True)\n",
    "    self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(out_chans)\n",
    "    self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu1(self.bn1(self.conv1(x)))\n",
    "    out = self.relu2(self.bn2(self.conv2(x)))\n",
    "    return out\n",
    "\n",
    "'''\n",
    "将该模块的卷积块修改为BasicBlock\n",
    "'''\n",
    "class UpConvBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, in_chans, bridge_chans_list, out_chans):\n",
    "    super(UpConvBlock, self).__init__()\n",
    "    self.up = nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2) # 上采样两倍\n",
    "    self.conv_block = BasicBlock(out_chans + sum(bridge_chans_list), out_chans, 1)\n",
    "#     self.conv_block = ConvBlock(out_chans + sum(bridge_chans_list), out_chans, 1)\n",
    "\n",
    "\n",
    "  def forward(self, x, bridge_list):\n",
    "    x = self.up(x)\n",
    "    x = torch.cat([x] + bridge_list, dim=1)\n",
    "    out = self.conv_block(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "o_ruMp_H3Foa"
   },
   "outputs": [],
   "source": [
    "class ResNet_UNetpp(nn.Module):\n",
    "\n",
    "  def __init__(self, in_chans=1, n_classes=2, backbone=resnet18):\n",
    "    super(ResNet_UNetpp, self).__init__()\n",
    "    \n",
    "    '''\n",
    "    利用__init__函数的最后一个参数backbone，来替换下面的三个ConvBlock\n",
    "    注意feat_chans要进行相应修改(借助expansion)，同时兼容resnet18/34/50/101/152\n",
    "    feat_chans中可以有相同的数字\n",
    "    '''\n",
    "#     feat_chans = [64, 128, 256]\n",
    "#     self.conv_x00 = ConvBlock(in_chans, feat_chans[0], 1)\n",
    "#     self.conv_x10 = ConvBlock(feat_chans[0], feat_chans[1], 2)\n",
    "#     self.conv_x20 = ConvBlock(feat_chans[1], feat_chans[2], 2)\n",
    "    self.backbone = backbone(in_chans)\n",
    "    expansion = self.backbone.block.expansion\n",
    "    feat_chans = [64, 64 * expansion, 128 * expansion]\n",
    "    \n",
    "\n",
    "    '''\n",
    "    以下网络结构不允许修改\n",
    "    '''\n",
    "    self.conv_x01 = UpConvBlock(feat_chans[1], [feat_chans[0]], feat_chans[0])\n",
    "    self.conv_x11 = UpConvBlock(feat_chans[2], [feat_chans[1]], feat_chans[1])\n",
    "    self.conv_x02 = UpConvBlock(feat_chans[1], [feat_chans[0], feat_chans[0]], feat_chans[0])\n",
    "    \n",
    "    self.cls_conv_x01 = nn.Conv2d(feat_chans[0], 2, kernel_size=1)\n",
    "    self.cls_conv_x02 = nn.Conv2d(feat_chans[0], 2, kernel_size=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    替换为backbone的输出\n",
    "    '''\n",
    "#     x00 = self.conv_x00(x)\n",
    "#     x10 = self.conv_x10(x00)\n",
    "#     x20 = self.conv_x20(x10)\n",
    "    x00, x10, x20 = self.backbone(x)\n",
    "    x01 = self.conv_x01(x10, [x00])\n",
    "    x11 = self.conv_x11(x20, [x10])\n",
    "    x02 = self.conv_x02(x11, [x00, x01])\n",
    "    out01 = self.cls_conv_x01(x01)\n",
    "    out02 = self.cls_conv_x02(x02)\n",
    "\n",
    "    '''\n",
    "    用以下代码打印backbone为resnet34和resnet50时的结果，并截图提交\n",
    "    '''\n",
    "    print('x00', x00.shape)\n",
    "    print('x10', x10.shape)\n",
    "    print('x20', x20.shape)\n",
    "    print('x01', x01.shape)\n",
    "    print('x11', x11.shape)\n",
    "    print('x02', x02.shape)\n",
    "    print('out01', out01.shape)\n",
    "    print('out02', out02.shape)\n",
    "\n",
    "    return out01, out02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wfWVhFms-e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x00 torch.Size([2, 64, 224, 224])\n",
      "x10 torch.Size([2, 64, 112, 112])\n",
      "x20 torch.Size([2, 128, 56, 56])\n",
      "x01 torch.Size([2, 64, 224, 224])\n",
      "x11 torch.Size([2, 64, 112, 112])\n",
      "x02 torch.Size([2, 64, 224, 224])\n",
      "out01 torch.Size([2, 2, 224, 224])\n",
      "out02 torch.Size([2, 2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 1, 224, 224), dtype=torch.float32)\n",
    "model = ResNet_UNetpp(in_chans=1, backbone=resnet34)\n",
    "y1, y2 = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_UNetpp_homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python_AI_CV",
   "language": "python",
   "name": "cv_ml_kr_skl_torch_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
